{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e134111",
   "metadata": {},
   "source": [
    "# A Notebook to Explore Multi-Answer QA Datasets\n",
    "... and to test tools for interacting with them\n",
    "\n",
    "Contains a few sections:\n",
    "1. Explore Natural Questions\n",
    "2. Explore AmbigQA\n",
    "3. Explore QAMPARI\n",
    "4. Load DPR retreiver checkpoint\n",
    "5. Use BM25 Search\n",
    "\n",
    "Note that AmbigQA section:\n",
    "- uses utils for seeing how many questions have no positive contexts\n",
    "- investigates how many annotations are conflicting (with examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5494e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "import json\n",
    "\n",
    "from multiqa_utils import retrieval_utils as ru\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6f9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/scratch/ddr8143/repos/DPR/downloads/data\"\n",
    "ambigqa_path = f\"{base_path}/ambigqa\"\n",
    "ambigqa_light_path = f\"{base_path}/ambigqa_light\"\n",
    "nq_path = f\"{base_path}/retriever\"\n",
    "qp_path = f\"{base_path}/qampari\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1934e40",
   "metadata": {},
   "source": [
    "## Explore Natural Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8903b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LICENSE  README  nq-adv-hn-train.json  nq-dev.json  nq-train.json"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh.ls(nq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f5f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_data = json.load(open(f\"{nq_path}/nq-dev.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e13322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ Keys: dict_keys(['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs'])\n",
      "\n",
      "question:  who sings does he love me with reba\n",
      "answers:  ['Linda Davis']\n"
     ]
    }
   ],
   "source": [
    "print(\"NQ Keys:\", nq_data[0].keys())\n",
    "print()\n",
    "for k in [\"question\", \"answers\"]:\n",
    "    print(k + \": \", nq_data[0][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aafa26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"Why Don't You Love Me (Beyoncé song)\", 'text': 'song. According to the lyrics of \"Why Don\\'t You Love Me\", Knowles impersonates a woman who questions her love interest about the reason for which he does not value her fabulousness, convincing him she\\'s the best thing for him as she sings: \"Why don\\'t you love me... when I make me so damn easy to love?... I got beauty... I got class... I got style and I got ass...\". The singer further tells her love interest that the decision not to choose her is \"entirely foolish\". Originally released as a pre-order bonus track on the deluxe edition of \"I Am...', 'score': 14.678405, 'title_score': 0, 'passage_id': '14525568'}\n"
     ]
    }
   ],
   "source": [
    "# Then look at a hard negative context structure\n",
    "print(nq_data[0][\"hard_negative_ctxs\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c754df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Great Lakes',\n",
       " 'text': 'Great Lakes The Great Lakes (), also called the Laurentian Great Lakes and the Great Lakes of North America, are a series of interconnected freshwater lakes located primarily in the upper mid-east region of North America, on the Canada–United States border, which connect to the Atlantic Ocean through the Saint Lawrence River. They consist of Lakes Superior, Michigan, Huron, Erie, and Ontario, although hydrologically, there are four lakes, Superior, Erie, Ontario, and Michigan-Huron. The lakes are interconnected by the Great Lakes Waterway. The Great Lakes are the largest group of freshwater lakes on Earth by total area, and second largest',\n",
       " 'score': 1000,\n",
       " 'title_score': 1,\n",
       " 'passage_id': '151960'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And lets look at the structure of a positive context\n",
    "nq_data[1][\"positive_ctxs\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a9423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720379f8",
   "metadata": {},
   "source": [
    "## Explore AmbigQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e17c43",
   "metadata": {},
   "source": [
    "**First there's the light data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dacbbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dev.json  train.json"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh.ls(ambigqa_light_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597ed25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abl_data = json.load(open(f\"{ambigqa_light_path}/dev.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a00163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABL Keys: dict_keys(['annotations', 'id', 'question'])\n"
     ]
    }
   ],
   "source": [
    "print(\"ABL Keys:\", abl_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8474737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= Example 0 ==================================\n",
      "{\n",
      "    \"annotations\": [\n",
      "        {\n",
      "            \"type\": \"singleAnswer\",\n",
      "            \"answer\": [\n",
      "                \"Tony Goldwyn\",\n",
      "                \"Goldwyn\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"id\": \"-807825952267713091\",\n",
      "    \"question\": \"Who plays the doctor in dexter season 1?\"\n",
      "}\n",
      "\n",
      "============================= Example 1 ==================================\n",
      "{\n",
      "    \"annotations\": [\n",
      "        {\n",
      "            \"type\": \"singleAnswer\",\n",
      "            \"answer\": [\n",
      "                \"usually continues uninterrupted until death\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"singleAnswer\",\n",
      "            \"answer\": [\n",
      "                \"constant\",\n",
      "                \"usually continues uninterrupted until death\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"id\": \"8266116451988110240\",\n",
      "    \"question\": \"How often does spermatogeneis\\u2014the production of sperm\\u2014occur?\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(f\"============================= Example {i} ==================================\")\n",
    "    print(json.dumps(abl_data[i], indent=4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced422e3",
   "metadata": {},
   "source": [
    "**Then there's all the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68a8b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data = json.load(open(f\"{ambigqa_path}/dev.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "033af30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB Keys: dict_keys(['viewed_doc_titles', 'used_queries', 'annotations', 'nq_answer', 'id', 'nq_doc_title', 'question'])\n"
     ]
    }
   ],
   "source": [
    "print(\"AB Keys:\", ab_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e8e53c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     q:   Who plays the doctor in dexter season 1?\n",
      "         SA| t: singleAnswer a: ['Tony Goldwyn', 'Goldwyn']\n",
      "     nqa: ['Tony Goldwyn']\n",
      "1\n",
      "     q:   How often does spermatogeneis—the production of sperm—occur?\n",
      "         SA| t: singleAnswer a: ['usually continues uninterrupted until death']\n",
      "         SA| t: singleAnswer a: ['constant', 'usually continues uninterrupted until death']\n",
      "     nqa: ['74 days']\n",
      "2\n",
      "     q:   When was the first remote control tv invented?\n",
      "         SA| t: singleAnswer a: ['1950']\n",
      "         SA| t: singleAnswer a: ['1950']\n",
      "     nqa: ['1950']\n",
      "3\n",
      "     q:   Why did the st louis cardinals move to arizona?\n",
      "         SA| t: singleAnswer a: ['mediocrity of the Cardinals,a then-21-year-old stadium,game attendance to dwindle']\n",
      "         MA| t: [['overall mediocrity of the Cardinals'], ['old stadium'], ['game attendance to dwindle']]\n",
      "     nqa: ['1988']\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n",
    "    print(\"     q:  \", ab_data[i][\"question\"])\n",
    "    for a in ab_data[i][\"annotations\"]:\n",
    "        if \"answer\" in a:\n",
    "            print(\"         SA| t:\", a[\"type\"], \"a:\", a[\"answer\"])\n",
    "        else:\n",
    "            print(\"         MA| t:\", [dd[\"answer\"] for dd in a[\"qaPairs\"]])\n",
    "    \n",
    "    print(\"     nqa:\", ab_data[i][\"nq_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ba6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace13714",
   "metadata": {},
   "source": [
    "**Then, lets look at a result from BM25 to see how many questions have no postive contexts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e97e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits | No Positive Contexts Retrieved\n",
      "---- | ------------------------------\n",
      " 100 | 319/2002 (15.93%)\n",
      " 400 | 221/2002 (11.04%)\n",
      "1000 | 170/2002 (8.49%)\n"
     ]
    }
   ],
   "source": [
    "ambigqa_bm25_outdir = \"/scratch/ddr8143/repos/pyserini/runs\"\n",
    "ambigqa_bm25_pathname_fxn = lambda hits: ru.bm25_out_name(ambigqa_bm25_outdir, \"ambigqa_light\", \"dev\", hits)\n",
    "ru.display_no_positive([100, 400, 1000], ambigqa_bm25_pathname_fxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c23e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a1693d",
   "metadata": {},
   "source": [
    "**And lets look at the annotation quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84d693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = '/'.join([ambigqa_light_path, \"train.json\"])\n",
    "train_dataset = json.load(open(train_dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84603b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 questions in the training set have more than one annotation\n",
      "\n",
      "For example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'annotations': [{'type': 'singleAnswer', 'answer': ['New Delhi']},\n",
       "  {'type': 'singleAnswer',\n",
       "   'answer': ['New Delhi', 'New Delhi, India', 'Delhi']}],\n",
       " 'id': '3978528412752837293',\n",
       " 'question': \"India's first ever all india institute of ayurveda has come up in which city?\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strange_ds = []\n",
    "for d in train_dataset:\n",
    "    if len(d[\"annotations\"]) > 1:\n",
    "        strange_ds.append(d)\n",
    "print(f\"{len(strange_ds)} questions in the training set have more than one annotation\")\n",
    "print()\n",
    "print(\"For example:\")\n",
    "strange_ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8cf0353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogri language is spoken in which state of india?\n",
      "       A: ['Jammu and Kashmir, Himachal Pradesh, Punjab', 'Jammu and Kashmir, Himachal Pradesh, Punjab']\n",
      "       A: {'Jammu and Kashmir, Himachal Pradesh, Punjab'}\n",
      "Who plays granny on once upon a time?\n",
      "       A: ['Beverley Elliott', 'Elliott', 'Beverley Elliott']\n",
      "       A: {'Beverley Elliott', 'Elliott'}\n",
      "India's first ever all india institute of ayurveda has come up in which city?\n",
      "       A: ['New Delhi', 'New Delhi', 'New Delhi, India', 'Delhi']\n",
      "       A: {'Delhi', 'New Delhi', 'New Delhi, India'}\n",
      "Type of epithelial tissue containing cells that can change shapes as the tissue stretches?\n",
      "       A: ['Transitional epithelium', 'a type of stratified epithelium', 'Transitional epithelium']\n",
      "       A: {'Transitional epithelium', 'a type of stratified epithelium'}\n",
      "When did the ranch season 2 come out?\n",
      "       A: ['June 16, 2017', 'December 15, 2017', 'June 16, 2017', '2017', 'December 15, 2017', '2017']\n",
      "       A: {'December 15, 2017', '2017', 'June 16, 2017'}\n",
      "Who is the first rail minister of india?\n",
      "       A: ['John Mathai', 'Mathai', 'Dr. John Matthai', 'John Mathai']\n",
      "       A: {'Dr. John Matthai', 'Mathai', 'John Mathai'}\n",
      "Who voices the prince in happily never after?\n",
      "       A: ['Warburton', 'Patrick Warburton', 'Patrick Warburton']\n",
      "       A: {'Patrick Warburton', 'Warburton'}\n",
      "Where is the tv show hell's kitchen located?\n",
      "       A: ['Los Angeles, California', 'modified warehouse in Los Angeles', 'Los Angeles', 'Las Vegas, Nevada', \"Hell's Kitchen restaurant\", 'Las Vegas', 'United Kingdom', 'London', 'specially constructed London restaurant-kitchen', 'Los Angeles', 'Las Vegas, Nevada', 'London']\n",
      "       A: {\"Hell's Kitchen restaurant\", 'Las Vegas, Nevada', 'Los Angeles, California', 'Las Vegas', 'modified warehouse in Los Angeles', 'London', 'United Kingdom', 'specially constructed London restaurant-kitchen', 'Los Angeles'}\n",
      "When did the canon rebel t3i come out?\n",
      "       A: ['February 7, 2011', '2011', '7 February 2011']\n",
      "       A: {'2011', '7 February 2011', 'February 7, 2011'}\n",
      "What is the oldest beer brewery in america?\n",
      "       A: ['Yuengling', 'D. G. Yuengling & Son', 'D.G. Yuengling & Son Brewing Complex', '\"Eagle Brewery\"', 'D.G. Yuengling & Son']\n",
      "       A: {'D. G. Yuengling & Son', 'D.G. Yuengling & Son Brewing Complex', 'D.G. Yuengling & Son', 'Yuengling', '\"Eagle Brewery\"'}\n"
     ]
    }
   ],
   "source": [
    "# Some more examples that have conflicting annotations:\n",
    "for d in strange_ds[:10]:\n",
    "    answers = []\n",
    "    for anns in d['annotations']:\n",
    "        if anns['type'] == 'multipleQAs':\n",
    "            for qap in anns['qaPairs']:\n",
    "                answers.extend(qap['answer'])\n",
    "        else:\n",
    "            answers.extend(anns['answer'])\n",
    "    print(d[\"question\"])\n",
    "    print(\"       A:\", answers)\n",
    "    print(\"       A:\", set(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc7e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56717c76",
   "metadata": {},
   "source": [
    "## Explore QAMPARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3fe0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dev_data.jsonl\ttest_data.jsonl  train_data.jsonl"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh.ls(qp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c19241eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_data = []\n",
    "for l in open(f\"{qp_path}/train_data.jsonl\").readlines():\n",
    "    qp_data.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "468ed6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['entities', 'question_text', 'answer_list', 'qid'])\n",
      "\n",
      "0__wikidata_simple__train: Which movie, clip, TV show etc. had Chezhiyan as director of photography?\n",
      "\n",
      "{'entity_url': 'https://en.wikipedia.org/wiki/Chezhiyan', 'entity_text': 'Chezhiyan', 'aliases': ['Chezhiyan']}\n",
      "\n",
      "{'answer_text': 'To Let', 'aid': '0__wikidata_simple__train__0', 'aliases': ['To Let'], 'answer_url': 'https://en.wikipedia.org/wiki/To_Let_(film)', 'proof': [{'proof_text': 'To let is a 2017 indian tamil-language drama film written, directed and filmed by chezhiyan.', 'found_in_url': 'https://en.wikipedia.org/wiki/To_Let_(film)', 'pid': '0__wikidata_simple__train__0__0'}]}\n"
     ]
    }
   ],
   "source": [
    "print(qp_data[0].keys())\n",
    "print()\n",
    "print(f\"{qp_data[0]['qid']}: {qp_data[0]['question_text']}\")\n",
    "print()\n",
    "print(qp_data[0]['entities'][0])\n",
    "print()\n",
    "print(qp_data[0]['answer_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a2b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd324412",
   "metadata": {},
   "source": [
    "## Load a DPR Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a5cb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a709d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n"
     ]
    }
   ],
   "source": [
    "the_model = torch.load(\n",
    "    \"/scratch/ddr8143/repos/DPR/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp\",\n",
    "    map_location=torch.device('cpu'),\n",
    ")\n",
    "print(the_model.keys())\n",
    "model_dict = the_model[\"model_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd9128fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ctx_model', 'question_model'}\n",
      "\n",
      "Question model\n",
      ">> question_model.embeddings.word_embeddings.weight\n",
      ">> question_model.embeddings.position_embeddings.weight\n",
      ">> question_model.embeddings.token_type_embeddings.weight\n",
      ">> question_model.embeddings.LayerNorm.weight\n",
      ">> question_model.embeddings.LayerNorm.bias\n",
      "\n",
      "Context model\n",
      ">> ctx_model.embeddings.word_embeddings.weight\n",
      ">> ctx_model.embeddings.position_embeddings.weight\n",
      ">> ctx_model.embeddings.token_type_embeddings.weight\n",
      ">> ctx_model.embeddings.LayerNorm.weight\n",
      ">> ctx_model.embeddings.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "# Get the model structure:\n",
    "top_levels = set([k.split(\".\")[0] for k in model_dict.keys()])\n",
    "print(top_levels)\n",
    "q_model_ks = [k for k in model_dict.keys() if 'question_model' in k and 'embeddings' in k]\n",
    "c_model_ks = [k for k in model_dict.keys() if 'ctx_model' in k and 'embeddings' in k]\n",
    "print()\n",
    "print(\"Question model\")\n",
    "for k in q_model_ks:\n",
    "    print(\">>\", k)\n",
    "print()\n",
    "print(\"Context model\")\n",
    "for k in c_model_ks:\n",
    "    print(\">>\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4896cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['embeddings.position_ids',\n",
       " 'embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.LayerNorm.weight',\n",
       " 'embeddings.LayerNorm.bias']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we can also load the base model\n",
    "from transformers import BertModel, BertConfig\n",
    "cfg = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "hf_bert = model.state_dict()\n",
    "model_embed_ks = [k for k in hf_bert.keys() if \"embeddings\" in k]\n",
    "model_embed_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28529f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8349b3c",
   "metadata": {},
   "source": [
    "## Test using BM25 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6434f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.index.lucene import IndexReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b35df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LuceneSearcher.list_prebuilt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5597fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index wikipedia-dpr.\n",
      "/home/ddr8143/.cache/pyserini/indexes/index-wikipedia-dpr-20210120-d1b9e6.c28f3a56b2dfcef25bf3bf755c264d04 already exists, skipping download.\n",
      "Initializing wikipedia-dpr...\n"
     ]
    }
   ],
   "source": [
    "searcher = LuceneSearcher.from_prebuilt_index('wikipedia-dpr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e224794",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = searcher.search('hubble space telescope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4706213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 500264          17.42493, {  \"id\" : \"500264\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nHubble Space Telescope The Hubble S\n",
      " 2 500350          17.02356, {  \"id\" : \"500350\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nThese are often European in origin,\n",
      " 3 500368          16.58024, {  \"id\" : \"500368\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nvery narrow field—Lucky Cam, for ex\n",
      " 4 500266          16.45677, {  \"id\" : \"500266\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\ndata, while the Goddard Space Fligh\n",
      " 5 500367          16.37399, {  \"id\" : \"500367\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nthe visible, ultraviolet, and infra\n",
      " 6 500265          16.27738, {  \"id\" : \"500265\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nultraviolet, visible, and near infr\n",
      " 7 500362          16.26323, {  \"id\" : \"500362\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\n(SCRS). , the Trump Administration \n",
      " 8 14244283        16.20809, {  \"id\" : \"14244283\",  \"contents\" : \"\\\"Hubble (film)\\\"\\nHubble (film) Hubble (also known as Hubble\n",
      " 9 500363          16.19711, {  \"id\" : \"500363\",  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nobjects generally older and farther\n",
      "10 2986290         16.19711, {  \"id\" : \"2986290\",  \"contents\" : \"\\\"John M. Grunsfeld\\\"\\ncommunicating space science topics to t\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    doc = searcher.doc(hits[i].docid)\n",
    "    docstr = doc.raw()[:100].replace(\"\\n\", \"\")\n",
    "    print(f'{i+1:2} {hits[i].docid:15} {hits[i].score:.5f}, {docstr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d57fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\" : \"500264\",\n",
      "  \"contents\" : \"\\\"Hubble Space Telescope\\\"\\nHubble Space Telescope The Hubble Space Telescope (HST) is a space telescope that was launched into low Earth orbit in 1990 and remains in operation. Although not the first space telescope, Hubble is one of the largest and most versatile and is well known as both a vital research tool and a public relations boon for astronomy. The HST is named after the astronomer Edwin Hubble and is one of NASA's Great Observatories, along with the Compton Gamma Ray Observatory, the Chandra X-ray Observatory and the Spitzer Space Telescope. With a mirror, Hubble's four main instruments observe in the near\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(hits[0].raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "760a1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m pyserini.search.lucene \\\n",
    "#   --index wikipedia-dpr \\\n",
    "#   --topics dpr-nq-test \\\n",
    "#   --output runs/run.dpr.nq-test.bm25.trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c75de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
